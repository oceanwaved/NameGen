{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oceanwaved/NameGen/blob/main/src/NameGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-LTNRNmIpjx"
      },
      "source": [
        "**Username Generator**\n",
        "\n",
        "by Oceanwave\n",
        "\n",
        "Runtime -> \"Run all\", scroll to bottom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9dc1_0UNCX4"
      },
      "outputs": [],
      "source": [
        "use_github_training_data = True\n",
        "use_github_model = True\n",
        "training = False\n",
        "download_model = False\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/oceanwaved/NameGen/main/data/63k_names.txt'\n",
        "model_url = 'https://raw.githubusercontent.com/oceanwaved/NameGen/main/models/model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BryPHm-QafjP"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Has array functions\n",
        "import numpy as np\n",
        "\n",
        "# For character array\n",
        "import string\n",
        "\n",
        "# To upload/download files\n",
        "from google.colab import files\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGMmsSa4amBq"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "num_layers = 1\n",
        "embedding_dim = 8\n",
        "num_epochs = 5\n",
        "learning_rate = 0.005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQZmtrz1aniC",
        "outputId": "68c7a4d5-6155-4c93-b036-7dddc8098e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Names\n",
            "['Elambo', 'ReDjionisuu', 'bustward', 'Avexster', 'MaFa', 'Mukuorbarius', 'Lineuzin', 'shaifman', 'Rosters', '4n4n']\n"
          ]
        }
      ],
      "source": [
        "# Get training data\n",
        "if use_github_training_data:\n",
        "    response = requests.get(data_url)\n",
        "    if response.status_code == 200:\n",
        "        usernames = [line.strip() for line in response.text.split('\\n')]\n",
        "    else:\n",
        "        print('Failed to download the file. Status code:', response.status_code)\n",
        "else:\n",
        "    # Upload data, format of one name per line\n",
        "    uploaded = files.upload()\n",
        "    file_name = next(iter(uploaded))\n",
        "    with open(file_name, 'r') as file:\n",
        "        usernames = [line.strip() for line in file]\n",
        "\n",
        "# Print 10 names\n",
        "np.random.shuffle(usernames)\n",
        "print(\"Example Names\")\n",
        "print(usernames[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUVT1Z7va10z"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "chars = list(string.ascii_letters + string.digits) + ['<SOS>', '<EOS>', '<PAD>']\n",
        "max_chars = len(max(usernames, key=len)) + 2\n",
        "\n",
        "# Lookup tables\n",
        "char_to_index = {char: index for index, char in enumerate(chars)}\n",
        "index_to_char = {index: char for index, char in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zltL43UjbH-4"
      },
      "outputs": [],
      "source": [
        "# Encode function\n",
        "def encode_sequence(sequence):\n",
        "    return [char_to_index[char] for char in sequence]\n",
        "\n",
        "# Pad function\n",
        "def pad_sequence(sequence, max_length):\n",
        "    return [char_to_index['<PAD>']] * (max_length - len(sequence)) + sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEh67lc0bQRq"
      },
      "outputs": [],
      "source": [
        "# Prepare Dataset\n",
        "class UsernameDataset(Dataset):\n",
        "    def __init__(self, usernames, max_length):\n",
        "        # Inputs, targets\n",
        "        self.X_data = []\n",
        "        self.y_data = []\n",
        "\n",
        "        # Add data\n",
        "        for username in usernames:\n",
        "            # Encode\n",
        "            sequence = ['<SOS>'] + list(username) + ['<EOS>']\n",
        "            encoded_sequence = encode_sequence(sequence)\n",
        "\n",
        "            # Put inputs and targets in dataset\n",
        "            for i in range(1, len(encoded_sequence)):\n",
        "                input_sequence = encoded_sequence[:i]\n",
        "                padded_input_sequence = pad_sequence(input_sequence, max_length)\n",
        "                next_char = encoded_sequence[i]\n",
        "                self.X_data.append(padded_input_sequence)\n",
        "                self.y_data.append(next_char)\n",
        "\n",
        "    # Used for DataLoader\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.X_data[index]\n",
        "        y = self.y_data[index]\n",
        "        return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long)   #Convert type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7jkRY9hbY9o"
      },
      "outputs": [],
      "source": [
        "# Create Dataset and DataLoaders\n",
        "dataset = UsernameDataset(usernames, max_chars)\n",
        "\n",
        "# Parameters\n",
        "test_ratio = 0.1\n",
        "total_size = len(dataset)\n",
        "test_size = int(test_ratio * total_size)\n",
        "train_size = total_size - test_size\n",
        "\n",
        "# Splitting the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders for train and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LYlFzvqB18n"
      },
      "outputs": [],
      "source": [
        "# Define input/output sizes\n",
        "input_size = len(chars)\n",
        "output_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x24IYaKPbZVm"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class UsernameGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, embedding_dim):\n",
        "        super(UsernameGenerator, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        # Fully Connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        out, hidden = self.lstm(embedded, hidden)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    # Reset memory for batch\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (\n",
        "            torch.zeros(num_layers, batch_size, hidden_size),\n",
        "            torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbRP2XU6fw7t"
      },
      "outputs": [],
      "source": [
        "# Make model\n",
        "model = UsernameGenerator(input_size, hidden_size, output_size, num_layers, embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e5GMvL_f8C8"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "# ~1 hr with default hyperparameters and 63k name dataset\n",
        "def train(model, train_loader, test_loader, num_epochs, batch_size, output_size):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_examples = 0\n",
        "\n",
        "        for (X_batch, y_batch) in tqdm(train_loader, desc=f\"epoch {epoch + 1}\"):\n",
        "            # Reset from last batch\n",
        "            optimizer.zero_grad()\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            # Get output\n",
        "            output, hidden = model(X_batch, hidden)\n",
        "\n",
        "            # Calculate loss\n",
        "            output = output.float()\n",
        "            one_hot = F.one_hot(y_batch, num_classes=output_size).float()\n",
        "            loss = criterion(output, one_hot)\n",
        "\n",
        "            # Step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Increment total loss\n",
        "            train_loss += loss.item() * batch_size\n",
        "            train_examples += batch_size\n",
        "\n",
        "        # Print total loss\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/train_examples}')\n",
        "\n",
        "        # Test\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_examples = 0\n",
        "        with torch.no_grad():\n",
        "            for (X_batch, y_batch) in tqdm(test_loader, desc=\"Validating\"):\n",
        "                hidden = model.init_hidden(batch_size)\n",
        "                output, hidden = model(X_batch, hidden)\n",
        "                output = output.float()\n",
        "                one_hot = F.one_hot(y_batch, num_classes=output_size).float()\n",
        "                loss = criterion(output, one_hot)\n",
        "                val_loss += loss.item() * batch_size\n",
        "                val_examples += batch_size\n",
        "\n",
        "        if(epoch + 1 == num_epochs):\n",
        "            print()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Test Loss: {val_loss/val_examples}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAgvzO9eAGI-",
        "outputId": "e5cc1796-6812-42b0-b961-9bf06d961598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Upload model (if applicable, skip if no model)\n",
        "if use_github_model:\n",
        "    response = requests.get(model_url)\n",
        "    if response.status_code == 200:\n",
        "        # Save the model file locally in Colab\n",
        "        with open('model.pth', 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        model.load_state_dict(torch.load('model.pth'))\n",
        "        print('Model downloaded and saved successfully.')\n",
        "    else:\n",
        "        print('Failed to download the model. Status code:', response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNTWyLIDuZCJ"
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "if training:\n",
        "    train(model, train_loader, test_loader, num_epochs, batch_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74-5qwSPfzjH"
      },
      "outputs": [],
      "source": [
        "# Username Generator\n",
        "def generate_username(model, seed=\"\", minimum_length=3):\n",
        "    # Go out of training mode\n",
        "    model.eval()\n",
        "\n",
        "    # <SOS> at start of the sequence\n",
        "    sequence = ['<SOS>'] + list(seed)\n",
        "\n",
        "    # No tweaks\n",
        "    with torch.no_grad():\n",
        "        # Reset memory (only one in batch)\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "        # While username (minus <SOS>) is less than max chars - 2\n",
        "        while len(sequence) - 1 < max_chars - 2:\n",
        "            # Convert type\n",
        "            encoded_sequence = encode_sequence(sequence)\n",
        "            padded_sequence = pad_sequence(encoded_sequence, max_chars)\n",
        "            X_tensor = torch.tensor(padded_sequence, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "            # Get distribution\n",
        "            output, hidden = model(X_tensor, hidden)\n",
        "            probabilities = F.softmax(output.squeeze(), dim=-1)\n",
        "\n",
        "            # Exclude SOS and PAD, EOS if < 3\n",
        "            indices_to_zero = [char_to_index['<SOS>'], char_to_index['<PAD>']]\n",
        "            if len(sequence) - 1 < minimum_length:\n",
        "                indices_to_zero.append(char_to_index['<EOS>'])\n",
        "            probabilities[indices_to_zero] = 0.0\n",
        "\n",
        "            # Renormalize\n",
        "            if probabilities.sum() == 0:\n",
        "                valid_indices = [i for i in range(len(probabilities)) if i not in indices_to_zero]\n",
        "                probabilities[valid_indices] = 1 / len(valid_indices)\n",
        "            probabilities /= probabilities.sum()\n",
        "\n",
        "            # Find next character\n",
        "            next_char_index = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = index_to_char[next_char_index]\n",
        "\n",
        "            # Break for EOS\n",
        "            if next_char == '<EOS>':\n",
        "                break\n",
        "\n",
        "            # Add character to sequence\n",
        "            sequence.append(next_char)\n",
        "\n",
        "        # Go back into training mode\n",
        "        model.train()\n",
        "\n",
        "        # Exclude <SOS>\n",
        "        return ''.join(sequence[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dOYxNYyf5RO",
        "outputId": "9318f9c3-9358-4c75-cb8b-174662b780d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nissegg\n",
            "rowu01\n",
            "Lolwoy8\n",
            "Natatwock\n",
            "LancyWaia\n",
            "Bigbigpander1\n",
            "PeMageblock\n",
            "myry819\n",
            "Lilitai\n",
            "TheFowShaman\n",
            "mrnrora\n",
            "BlackBr1n\n",
            "EJCIIIs\n",
            "ElLordGuerren\n",
            "Mejlvangth\n",
            "FoxyBox2234\n",
            "voloke\n",
            "NurpllalieI\n",
            "DarkSouls\n",
            "Aquallemight\n",
            "Loqnized\n",
            "Archerer\n",
            "WiWTRS1\n",
            "CreatCittt\n",
            "Hellen\n",
            "MoRou\n",
            "KebuKu\n",
            "9gsylen\n",
            "Illonrim133\n",
            "SoulTV\n",
            "KillerTHU\n",
            "tptdug\n",
            "WispbalfRor3\n",
            "Laydos\n",
            "Bugla\n",
            "sharpua\n",
            "Copet\n",
            "Soulavini\n",
            "Dytygned\n",
            "ni6942032\n",
            "IKbones\n",
            "ESEROPBAA\n",
            "CakavElo2\n",
            "arucas\n",
            "xR1bIo\n",
            "Pedaitix\n",
            "duskythpolator\n",
            "kud0nwarrior\n",
            "Goomph\n",
            "Teomorgab\n"
          ]
        }
      ],
      "source": [
        "# Make 50 usernames\n",
        "username_list = []\n",
        "for i in range(50):\n",
        "    generated_username = generate_username(model)\n",
        "    # generated_username = generate_username(model, seed=\"Seed\")\n",
        "    username_list.append(generated_username)\n",
        "\n",
        "# Print them\n",
        "for username in username_list:\n",
        "    print(username)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmAkBx4-_9Zg"
      },
      "outputs": [],
      "source": [
        "# Save model for later\n",
        "if download_model:\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "    files.download('model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}